{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 15:20:56.231095: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-27 15:20:57.615394: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-27 15:20:58.978355: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-27 15:20:58.980908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-27 15:21:14.719292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from global_config import logger, cfg\n",
    "from Classes.Models import get_model\n",
    "import os\n",
    "import math\n",
    "from obspy import Trace, Stream, UTCDateTime\n",
    "from Classes.Scaler import Scaler\n",
    "from Classes.Utils import one_prediction\n",
    "from seismonpy.core import SeismonStream\n",
    "from seismonpy.norsardb import Client\n",
    "from seismonpy.utils import convert_velocity_slowness, create_global_mongodb_object\n",
    "from seismonpy.array_analysis.beams import array_beam\n",
    "from seismonpy.auto.beams.beam_types import VerticalCoherentBeamRecipe, HorizontalCoherentBeamRecipe\n",
    "from seismonpy.io.mongodb.eventdb import MongoEventDataBase\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveClassifier:\n",
    "    def __init__(self, model, scaler, label_maps, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.label_maps = label_maps\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, trace):\n",
    "        \"\"\"\n",
    "        Performs event classification for a specified time range.\n",
    "        Args:\n",
    "            start (UTCDateTime): Start time of the time range.\n",
    "            end (UTCDateTime): End time of the time range.\n",
    "            step (float): Step size in seconds.\n",
    "        Returns:\n",
    "            Predicted class.\n",
    "        \"\"\"\n",
    "        print(\"trace shape pre t: \", trace.shape)\n",
    "        trace = trace.T\n",
    "        print(\"trace shape post t: \", trace.shape)\n",
    "        X = self.prepare_multiple_intervals(trace)\n",
    "        for x in X:\n",
    "            print(\"x shape: \", x.shape)\n",
    "        X = [self.local_minmax(x) for x in X]\n",
    "        X = np.array(X)\n",
    "        # TODO: Figure out the logic for this\n",
    "        yhats, yprobas, final_yhat, mean_proba = self.ensamble_predict(self.model, X)\n",
    "\n",
    "        return final_yhat, mean_proba, yhats, yprobas, X\n",
    "    \n",
    "    def prepare_multiple_intervals(self, trace):\n",
    "        traces = []\n",
    "        # Creates equally sized intervals of the trace, using the user defined step size.\n",
    "        for start in range(0, len(trace) - (cfg.live.length*cfg.live.sample_rate)+1, (cfg.live.step*cfg.live.sample_rate)+1):\n",
    "            traces.append(trace[start:(start+cfg.live.length*cfg.live.sample_rate)+1])\n",
    "        return traces\n",
    "\n",
    "    def ensamble_predict(self, model, X):\n",
    "        # Basic ensamble prediction for the input data.\n",
    "        # TODO: Consider weighing predictions higher around the center (assuming thats where the pick is).\n",
    "        yhats, probas = [], {\"detector\": [], \"classifier\": []}\n",
    "        for x in X:\n",
    "            yhat, proba = one_prediction(model, x, self.label_maps)\n",
    "            yhats.append(yhat)\n",
    "            probas[\"detector\"].append(proba[\"detector\"])\n",
    "            probas[\"classifier\"].append(proba[\"classifier\"])\n",
    "        final_yhat = np.mean(yhats, axis=0)\n",
    "        mean_proba = {\"detector\": np.mean(probas[\"detector\"], axis=0), \"classifier\": np.mean(probas[\"classifier\"], axis=0)}\n",
    "        return yhats, probas, final_yhat, mean_proba\n",
    "    \n",
    "    def local_minmax(self, trace):\n",
    "        mmax = np.max(trace)\n",
    "        mmin = np.min(trace)\n",
    "        return (trace - mmin) / (mmax - mmin)\n",
    "        \n",
    "    \n",
    "class ClassifyGBF:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def get_beam(self, start, end, picks, inventory):\n",
    "        p_vel = cfg.live.p_vel\n",
    "        s_vel = cfg.live.s_vel\n",
    "        edge = cfg.live.edge\n",
    "        startt = start - edge\n",
    "        endt = end + edge\n",
    "        warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "        # TODO Fix the startt stuff.... \n",
    "        baz = self.average_bazimuth(picks)\n",
    "        try:\n",
    "            comp = 'BH*'\n",
    "            zcomp = '*Z'\n",
    "            tcomp = '*T'\n",
    "            rcomp = '*R'\n",
    "            if start < UTCDateTime('2014-09-19T00:00:00'):\n",
    "                comp = 's*'\n",
    "                zcomp = '*z'\n",
    "\n",
    "            stream = Client().get_waveforms(\n",
    "                'AR*', comp, starttime=startt - edge, endtime=endt + edge, sampling_rate_tolerance=0.5\n",
    "            )\n",
    "\n",
    "            stream = self.correct_trace_start_times(stream)\n",
    "\n",
    "            # Check for masked data, NaN values in traces\n",
    "            # Remove traces with more than 5 s masked\n",
    "            masked_traces = []\n",
    "            for tr in stream.traces:\n",
    "                if isinstance(tr.data, np.ma.masked_array):\n",
    "                    time_filled = tr.stats.delta* np.sum(tr.data.mask)\n",
    "                    if time_filled > 5.0:\n",
    "                        print(f'{time_filled:.4f} s of trace data masked, dropping trace - {tr.stats.starttime.__str__()}')\n",
    "                        masked_traces.append(tr)\n",
    "                    else:\n",
    "                        tr.data = tr.data.filled(0.0)\n",
    "                        print(f'{time_filled:.4f} s of trace data masked, filling with zeros - {tr.stats.starttime.__str__()}')\n",
    "\n",
    "                num_nans = np.sum(np.isnan(tr.data))\n",
    "                if num_nans > 0:\n",
    "                    time_containing_nans = num_nans*tr.stats.delta\n",
    "                    if time_containing_nans > 5.0:\n",
    "                        print(f'{time_containing_nans:.4f} s of trace has NaNs, dropping trace - {tr.stats.starttime.__str__()}')\n",
    "                        masked_traces.append(tr)\n",
    "                    else:\n",
    "                        tr.data = np.nan_to_num(tr.data)\n",
    "                        print(f'{time_containing_nans:.4f} s of trace has NaNs, filling with zeros - {tr.stats.starttime.__str__()}')\n",
    "\n",
    "\n",
    "            for tr in masked_traces:\n",
    "                if tr in stream.traces: # May have been removed already\n",
    "                    stream.remove(tr)\n",
    "            \n",
    "            if len(stream) == 0:\n",
    "                raise RuntimeError('Stream has no remaining traces')\n",
    "\n",
    "            stream.detrend('demean')\n",
    "            stream.taper(max_percentage=None, max_length=edge, type='cosine', halfcosine=True)\n",
    "            stream.filter('highpass', freq=1.5)\n",
    "            stream.resample(cfg.live.sample_rate)\n",
    "            #print(\"SAMPLE RATE:\", cfg.live.sample_rate)\n",
    "            stream.rotate('NE->RT', back_azimuth=baz, inventory=inventory)\n",
    "\n",
    "            p_time_delays = inventory.beam_time_delays(baz, p_vel)\n",
    "            p_beam_z = stream.select(channel=zcomp).create_beam(p_time_delays)\n",
    "            p_beam_z.stats.channel = 'P-beam, Z'\n",
    "\n",
    "            s_time_delays = inventory.beam_time_delays(baz, s_vel)\n",
    "            s_beam_t = stream.select(channel=tcomp).create_beam(s_time_delays)\n",
    "            s_beam_t.stats.channel = 'S-beam, T'\n",
    "            s_beam_r = stream.select(channel=rcomp).create_beam(s_time_delays)\n",
    "            s_beam_r.stats.channel = 'S-beam, R'\n",
    "\n",
    "            p_beam_z.trim(start, end)\n",
    "            s_beam_t.trim(start, end)\n",
    "            s_beam_r.trim(start, end)\n",
    "            \n",
    "            filter_name = cfg.filters.highpass_or_bandpass\n",
    "            if filter_name == \"highpass\":\n",
    "                stream.filter('highpass', freq = cfg.filters.high_kwargs.high_freq)\n",
    "            if filter_name == \"bandpass\":\n",
    "                stream.filter('bandpass', freqmin=cfg.filters.band_kwargs.min, freqmax=cfg.filters.band_kwargs.max)\n",
    "\n",
    "            stream = Stream([p_beam_z, s_beam_t, s_beam_r])\n",
    "            tracedata = np.array([p_beam_z.data, s_beam_t.data, s_beam_r.data])\n",
    "\n",
    "            return tracedata, stream\n",
    "\n",
    "        except Exception as exc:\n",
    "            print('ERROR: {} - {}'.format(start, exc))\n",
    "            return str(type(exc)) + str(exc), None\n",
    "\n",
    "    def correct_trace_start_times(self, stream, max_delta=0.15):\n",
    "        \"\"\"\n",
    "        For old data the traces might have tiny offset in start time, which breaks\n",
    "        beamforming. Adjust this manually.\n",
    "        Remove traces with diff > max_delta\n",
    "        \"\"\"\n",
    "        sts = [tr.stats.starttime for tr in stream.traces]\n",
    "        most_common = np.unique(sts)[0]\n",
    "\n",
    "        for tr in stream.traces:\n",
    "            this_starttime = tr.stats.starttime\n",
    "            if this_starttime != most_common:\n",
    "                if abs(this_starttime - most_common) <= max_delta:\n",
    "                    tr.stats.starttime = most_common\n",
    "                else:\n",
    "                    print('Removing trace:', tr)\n",
    "                    stream.remove(tr)\n",
    "        \n",
    "        return stream\n",
    "    \n",
    "    def get_data_to_predict(self, starttime, endtime):\n",
    "        filtered_events, inventory = self.get_array_picks(starttime, endtime, cfg.live.array)\n",
    "        print(\"Number of filtered events: \", len(filtered_events))\n",
    "        print(f\"Inventory: {inventory}\")\n",
    "        starttimes, endtimes = self.transform_events_to_start_and_end_times(filtered_events)\n",
    "        print(f\"starttimes: {starttimes}\")\n",
    "        tracedata, streams = [], []\n",
    "        for i, (starttime, endtime) in enumerate(zip(starttimes, endtimes)):\n",
    "            traced, stream = self.get_beam(starttime, endtime, filtered_events[i], inventory)\n",
    "            if traced is not isinstance(traced, str):\n",
    "                tracedata.append(traced)\n",
    "                streams.append(stream)\n",
    "        return tracedata, streams\n",
    "\n",
    "    \n",
    "    def average_bazimuth(self, picks):\n",
    "        bazimuths = [pick['backazimuth'] for pick in picks]\n",
    "        # Convert each azimuth to radians\n",
    "        radian_bazimuths = [math.radians(az) for az in bazimuths]\n",
    "        \n",
    "        # Calculate mean of sin and cos\n",
    "        mean_sin = sum(math.sin(az) for az in radian_bazimuths) / len(radian_bazimuths)\n",
    "        mean_cos = sum(math.cos(az) for az in radian_bazimuths) / len(radian_bazimuths)\n",
    "        \n",
    "        # Use atan2 to compute average azimuth in radians\n",
    "        average_bazimuth_rad = math.atan2(mean_sin, mean_cos)\n",
    "        \n",
    "        # Convert back to degrees, ensuring the result is within [0, 360)\n",
    "        average_bazimuth_deg = math.degrees(average_bazimuth_rad) % 360\n",
    "        \n",
    "        return average_bazimuth_deg\n",
    "    \n",
    "    def predict_gbf_event(self):\n",
    "        # Wrapper for predict function that handles GBF picks.\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def load_events(self, starttime: UTCDateTime, endtime: UTCDateTime, collection: str = \"gbf1440_large\", dbname: str = \"auto\",  \n",
    "        mongourl: str = \"mongo.norsar.no:27017\", mongodb_user: str = \"guest\", mongodb_password: str = \"guest\", \n",
    "        mongodb_authsource: str = \"test\"):\n",
    "        query = {\"$and\":\n",
    "            [\n",
    "                {\"origins.time\": {\"$gt\": starttime.isoformat()}},\n",
    "                {\"origins.time\": {\"$lt\": endtime.isoformat()}},\n",
    "                {\"picks.waveform_id.station_code\": \"ARCES\"}\n",
    "            ]\n",
    "            }\n",
    "        obj = create_global_mongodb_object(mongourl.split(\":\")[0], int(mongourl.split(\":\")[1]), mongodb_user, mongodb_password, mongodb_authsource)\n",
    "        db = MongoEventDataBase(obj[dbname], collection)\n",
    "        events = db.find_events(query, decode_result=True)\n",
    "        inventory = Client().get_array_inventory(cfg.live.array)\n",
    "        return events, inventory\n",
    "\n",
    "    def get_array_picks(self, starttime: UTCDateTime, endtime:UTCDateTime, station_code: str):\n",
    "        events, inventory = self.load_events(starttime, endtime)\n",
    "        # Filter events where ARCES made a detection\n",
    "        relevant_events = [event for event in events if any(pick.waveform_id.station_code == station_code for pick in event.picks)]\n",
    "        \n",
    "        # Extract only the ARCES-related picks from those events\n",
    "        nested_filtered_events = []\n",
    "        for event in relevant_events:\n",
    "            arces_picks = [pick for pick in event.picks if pick.waveform_id.station_code == station_code]\n",
    "            nested_filtered_events.append(arces_picks)\n",
    "            \n",
    "        return nested_filtered_events, inventory\n",
    "\n",
    "    def transform_events_to_start_and_end_times(self, filtered_events: list):\n",
    "        starttimes, endtimes = [], []\n",
    "        for event in filtered_events:\n",
    "            pick_times = [pick.time for pick in event]\n",
    "            start = min(pick_times)\n",
    "            end = max(pick_times)\n",
    "            duration = end - start\n",
    "            # Need to make sure the event is long enough to include the entire event + event buffer. \n",
    "            # Also \n",
    "            if duration < cfg.live.length:\n",
    "                missing_length = cfg.live.length - duration\n",
    "                start = start - missing_length/2\n",
    "                end = end + missing_length/2\n",
    "            # How to handle events that are too long for the model? \n",
    "            starttimes.append(start - cfg.live.event_buffer)\n",
    "            endtimes.append(end + cfg.live.event_buffer)\n",
    "\n",
    "        return starttimes, endtimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mARCES - 3668824431.py:10 - <module> - INFO: Input shape to the model: (9601, 3)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 15:22:27.890802: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mARCES - 3668824431.py:17 - <module> - INFO: Loaded model weights from /staff/tord/Workspace/arces_classification/output/models/cnn_dense_mute-sky-7934_20231013_090836_model_weights.h5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/obspy/core/util/attribdict.py:120: UserWarning: Setting attribute \"_id\" which is not a default attribute (\"event_descriptions\", \"comments\", \"picks\", \"amplitudes\", \"focal_mechanisms\", \"origins\", \"magnitudes\", \"station_magnitudes\", \"resource_id\", \"event_type\", \"event_type_certainty\", \"creation_info\", \"preferred_origin_id\", \"preferred_magnitude_id\", \"preferred_focal_mechanism_id\").\n",
      "  warnings.warn(msg)\n",
      "/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/obspy/core/util/attribdict.py:120: UserWarning: Setting attribute \"_originpositions\" which is not a default attribute (\"event_descriptions\", \"comments\", \"picks\", \"amplitudes\", \"focal_mechanisms\", \"origins\", \"magnitudes\", \"station_magnitudes\", \"resource_id\", \"event_type\", \"event_type_certainty\", \"creation_info\", \"preferred_origin_id\", \"preferred_magnitude_id\", \"preferred_focal_mechanism_id\").\n",
      "  warnings.warn(msg)\n",
      "/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/obspy/core/util/attribdict.py:120: UserWarning: Setting attribute \"_origintimes\" which is not a default attribute (\"event_descriptions\", \"comments\", \"picks\", \"amplitudes\", \"focal_mechanisms\", \"origins\", \"magnitudes\", \"station_magnitudes\", \"resource_id\", \"event_type\", \"event_type_certainty\", \"creation_info\", \"preferred_origin_id\", \"preferred_magnitude_id\", \"preferred_focal_mechanism_id\").\n",
      "  warnings.warn(msg)\n",
      "/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/obspy/core/util/attribdict.py:120: UserWarning: Setting attribute \"slowness_unit\" which is not a default attribute (\"event_descriptions\", \"comments\", \"picks\", \"amplitudes\", \"focal_mechanisms\", \"origins\", \"magnitudes\", \"station_magnitudes\", \"resource_id\", \"event_type\", \"event_type_certainty\", \"creation_info\", \"preferred_origin_id\", \"preferred_magnitude_id\", \"preferred_focal_mechanism_id\").\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered events:  34\n",
      "Inventory: Inventory created at 2023-10-27T13:22:55.227389Z\n",
      "\tCreated by: ObsPy 1.4.0\n",
      "\t\t    https://www.obspy.org\n",
      "\tSending institution: NO\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tNO\n",
      "\t\tStations (26):\n",
      "\t\t\tNO.ARA0 ()\n",
      "\t\t\tNO.ARA1 ()\n",
      "\t\t\tNO.ARA2 ()\n",
      "\t\t\tNO.ARA3 ()\n",
      "\t\t\tNO.ARB1 ()\n",
      "\t\t\tNO.ARB2 ()\n",
      "\t\t\tNO.ARB3 ()\n",
      "\t\t\tNO.ARB4 ()\n",
      "\t\t\tNO.ARB5 ()\n",
      "\t\t\tNO.ARC1 ()\n",
      "\t\t\tNO.ARC2 ()\n",
      "\t\t\tNO.ARC3 ()\n",
      "\t\t\tNO.ARC4 ()\n",
      "\t\t\tNO.ARC5 ()\n",
      "\t\t\tNO.ARC6 ()\n",
      "\t\t\tNO.ARC7 ()\n",
      "\t\t\tNO.ARD1 ()\n",
      "\t\t\tNO.ARD2 ()\n",
      "\t\t\tNO.ARD3 ()\n",
      "\t\t\tNO.ARD4 ()\n",
      "\t\t\tNO.ARD5 ()\n",
      "\t\t\tNO.ARD6 ()\n",
      "\t\t\tNO.ARD7 ()\n",
      "\t\t\tNO.ARD8 ()\n",
      "\t\t\tNO.ARD9 ()\n",
      "\t\t\tNO.ARE0 ()\n",
      "\t\tChannels (280):\n",
      "\t\t\tNO.ARA0..BDF, NO.ARA0..BHZ, NO.ARA0..BHN, NO.ARA0..BHE, \n",
      "\t\t\tNO.ARA0..HHZ, NO.ARA0..HHN, NO.ARA0..HHE, NO.ARA0.SE.SHE (3x), \n",
      "\t\t\tNO.ARA0.SN.SHN (3x), NO.ARA0.SZ.SHZ (3x), NO.ARA1..BDF (2x), \n",
      "\t\t\tNO.ARA1..BHZ, NO.ARA1..BHN, NO.ARA1..BHE, NO.ARA1..HHZ, \n",
      "\t\t\tNO.ARA1..HHN, NO.ARA1..HHE, NO.ARA1.SZ.SHZ (3x), NO.ARA2..BDF (2x)\n",
      "\t\t\tNO.ARA2..BHZ, NO.ARA2..BHN, NO.ARA2..BHE, NO.ARA2..HHZ, \n",
      "\t\t\tNO.ARA2..HHN, NO.ARA2..HHE, NO.ARA2.SZ.SHZ (3x), NO.ARA3..BDF, \n",
      "\t\t\tNO.ARA3..HHZ, NO.ARA3..HHN, NO.ARA3..HHE, NO.ARB1..BDF, \n",
      "\t\t\tNO.ARB1..HHZ, NO.ARB1..HHN, NO.ARB1..HHE, NO.ARB2..BDF (2x), \n",
      "\t\t\tNO.ARB2..BHZ, NO.ARB2..BHN, NO.ARB2..BHE, NO.ARB2..HHZ, \n",
      "\t\t\tNO.ARB2..HHN, NO.ARB2..HHE, NO.ARB2.SZ.SHZ (3x), NO.ARB3..BDF (2x)\n",
      "\t\t\tNO.ARB3..BHZ, NO.ARB3..BHN, NO.ARB3..BHE, NO.ARB3..HHZ, \n",
      "\t\t\tNO.ARB3..HHN, NO.ARB3..HHE, NO.ARB3.SZ.SHZ (3x), NO.ARB4..BDF, \n",
      "\t\t\tNO.ARB4..HHZ, NO.ARB4..HHN, NO.ARB4..HHE, NO.ARB5..BDF, \n",
      "\t\t\tNO.ARB5..HHZ, NO.ARB5..HHN, NO.ARB5..HHE, NO.ARC1..BHZ, \n",
      "\t\t\tNO.ARC1..BHN, NO.ARC1..BHE, NO.ARC1..HHZ, NO.ARC1..HHN, \n",
      "\t\t\tNO.ARC1..HHE, NO.ARC1.SZ.SHZ (3x), NO.ARC2..BHZ (2x), \n",
      "\t\t\tNO.ARC2..BHN (2x), NO.ARC2..BHE (2x), NO.ARC2..HHZ, NO.ARC2..HHN, \n",
      "\t\t\tNO.ARC2..HHE, NO.ARC2.SE.SHE (3x), NO.ARC2.SN.SHN (3x), \n",
      "\t\t\tNO.ARC2.SZ.SHZ (3x), NO.ARC3..BHZ (2x), NO.ARC3..BHN (2x), \n",
      "\t\t\tNO.ARC3..BHE (2x), NO.ARC3..HHZ, NO.ARC3..HHN, NO.ARC3..HHE, \n",
      "\t\t\tNO.ARC3.SZ.SHZ (3x), NO.ARC4..BHZ, NO.ARC4..BHN, NO.ARC4..BHE, \n",
      "\t\t\tNO.ARC4..HHZ, NO.ARC4..HHN, NO.ARC4..HHE, NO.ARC4.SE.SHE (3x), \n",
      "\t\t\tNO.ARC4.SN.SHN (3x), NO.ARC4.SZ.SHZ (3x), NO.ARC5..BHZ, \n",
      "\t\t\tNO.ARC5..BHN, NO.ARC5..BHE, NO.ARC5..HHZ, NO.ARC5..HHN, \n",
      "\t\t\tNO.ARC5..HHE, NO.ARC5.SZ.SHZ (3x), NO.ARC6..BHZ, NO.ARC6..BHN, \n",
      "\t\t\tNO.ARC6..BHE, NO.ARC6..HHZ, NO.ARC6..HHN, NO.ARC6..HHE, \n",
      "\t\t\tNO.ARC6.SZ.SHZ (3x), NO.ARC7..BHZ, NO.ARC7..BHN, NO.ARC7..BHE, \n",
      "\t\t\tNO.ARC7..HHZ, NO.ARC7..HHN, NO.ARC7..HHE, NO.ARC7.SE.SHE (3x), \n",
      "\t\t\tNO.ARC7.SN.SHN (3x), NO.ARC7.SZ.SHZ (3x), NO.ARD1..BHZ, \n",
      "\t\t\tNO.ARD1..BHN, NO.ARD1..BHE, NO.ARD1..HHZ, NO.ARD1..HHN, \n",
      "\t\t\tNO.ARD1..HHE, NO.ARD1.SZ.SHZ (3x), NO.ARD2..BHZ, NO.ARD2..BHN, \n",
      "\t\t\tNO.ARD2..BHE, NO.ARD2..HHZ, NO.ARD2..HHN, NO.ARD2..HHE, \n",
      "\t\t\tNO.ARD2.SZ.SHZ (3x), NO.ARD3..BHZ, NO.ARD3..BHN, NO.ARD3..BHE, \n",
      "\t\t\tNO.ARD3..HHZ, NO.ARD3..HHN, NO.ARD3..HHE, NO.ARD3.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARD4..BHZ, NO.ARD4..BHN, NO.ARD4..BHE, NO.ARD4..HHZ, \n",
      "\t\t\tNO.ARD4..HHN, NO.ARD4..HHE, NO.ARD4.SZ.SHZ (3x), NO.ARD5..BHZ, \n",
      "\t\t\tNO.ARD5..BHN, NO.ARD5..BHE, NO.ARD5..HHZ, NO.ARD5..HHN, \n",
      "\t\t\tNO.ARD5..HHE, NO.ARD5.SZ.SHZ (3x), NO.ARD6..BHZ (3x), \n",
      "\t\t\tNO.ARD6..BHN (3x), NO.ARD6..BHE (3x), NO.ARD6..HHZ, NO.ARD6..HHN, \n",
      "\t\t\tNO.ARD6..HHE, NO.ARD6.SZ.SHZ (3x), NO.ARD7..BHZ, NO.ARD7..BHN, \n",
      "\t\t\tNO.ARD7..BHE, NO.ARD7..HHZ, NO.ARD7..HHN, NO.ARD7..HHE, \n",
      "\t\t\tNO.ARD7.SZ.SHZ (3x), NO.ARD8..BHZ, NO.ARD8..BHN, NO.ARD8..BHE, \n",
      "\t\t\tNO.ARD8..HHZ, NO.ARD8..HHN, NO.ARD8..HHE, NO.ARD8.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARD9..BHZ (2x), NO.ARD9..BHN (2x), NO.ARD9..BHE (2x), \n",
      "\t\t\tNO.ARD9..HHZ, NO.ARD9..HHN, NO.ARD9..HHE, NO.ARD9.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARE0..BHZ, NO.ARE0..BHN, NO.ARE0..BHE, NO.ARE0.BE.BHE (4x), \n",
      "\t\t\tNO.ARE0.BN.BHN (4x), NO.ARE0.BZ.BHZ (4x), NO.ARE0.HE.HHE, \n",
      "\t\t\tNO.ARE0.HN.HHN, NO.ARE0.HZ.HHZ, NO.ARE0.IE.MHE, NO.ARE0.IN.MHN, \n",
      "\t\t\tNO.ARE0.IZ.MHZ, NO.ARE0.LE.LHE, NO.ARE0.LN.LHN, NO.ARE0.LZ.LHZ, \n",
      "\t\t\tNO.ARE0.XH.EHZ, NO.ARE0.XH.EHN, NO.ARE0.XH.EHE\n",
      "starttimes: [UTCDateTime(2023, 9, 19, 12, 3, 22, 200000), UTCDateTime(2023, 9, 19, 11, 59, 10, 400000), UTCDateTime(2023, 9, 19, 12, 1, 44, 850000), UTCDateTime(2023, 9, 19, 12, 5, 11, 400000), UTCDateTime(2023, 9, 19, 12, 10, 8, 400000), UTCDateTime(2023, 9, 19, 12, 17, 59, 500000), UTCDateTime(2023, 9, 19, 12, 20, 57), UTCDateTime(2023, 9, 19, 12, 32, 3, 200000), UTCDateTime(2023, 9, 19, 12, 43, 35, 500000), UTCDateTime(2023, 9, 19, 12, 42, 19, 900000), UTCDateTime(2023, 9, 19, 12, 56, 2, 700000), UTCDateTime(2023, 9, 19, 12, 51, 28, 900000), UTCDateTime(2023, 9, 19, 12, 58, 52, 500000), UTCDateTime(2023, 9, 19, 13, 14, 13, 700000), UTCDateTime(2023, 9, 19, 13, 16, 6, 800000), UTCDateTime(2023, 9, 19, 13, 26, 54, 300000), UTCDateTime(2023, 9, 19, 13, 36, 37), UTCDateTime(2023, 9, 19, 13, 33, 51, 850000), UTCDateTime(2023, 9, 19, 13, 35, 3, 250000), UTCDateTime(2023, 9, 19, 13, 48, 59, 200000), UTCDateTime(2023, 9, 19, 13, 55, 55, 100000), UTCDateTime(2023, 9, 19, 14, 2, 10, 500000), UTCDateTime(2023, 9, 19, 14, 11, 44, 800000), UTCDateTime(2023, 9, 19, 14, 32, 37, 200000), UTCDateTime(2023, 9, 19, 14, 27, 5, 50000), UTCDateTime(2023, 9, 19, 14, 49, 11, 300000), UTCDateTime(2023, 9, 19, 15, 0, 44, 200000), UTCDateTime(2023, 9, 19, 15, 16, 54, 100000), UTCDateTime(2023, 9, 19, 15, 7, 24, 550000), UTCDateTime(2023, 9, 19, 15, 28, 35), UTCDateTime(2023, 9, 19, 15, 31, 13, 400000), UTCDateTime(2023, 9, 19, 15, 27, 37, 950000), UTCDateTime(2023, 9, 19, 15, 23, 41), UTCDateTime(2023, 9, 19, 15, 51, 43, 650000)]\n"
     ]
    }
   ],
   "source": [
    "detector_label_map = {0: \"noise\", 1: \"event\"}\n",
    "classifier_label_map = {0:\"earthquake\", 1:\"exlposion\"}\n",
    "label_maps = {\"detector\": detector_label_map, \"classifier\": classifier_label_map}\n",
    "\n",
    "\n",
    "input_shape = (cfg.live.length*cfg.live.sample_rate + 1, 3)\n",
    "detector_class_weight_dict = {\"noise\": 1, \"event\": 1}\n",
    "classifier_class_weight_dict = {\"earthquake\": 1, \"explosion\": 1}\n",
    "\n",
    "logger.info(\"Input shape to the model: \" + str(input_shape))\n",
    "classifier_metrics = [None]\n",
    "detector_metrics = [None]\n",
    "model = get_model(detector_label_map, classifier_label_map, detector_metrics, classifier_metrics, \n",
    "                   detector_class_weight_dict, classifier_class_weight_dict)\n",
    "model.build(input_shape=(None, *input_shape))  # Explicitly building the model here\n",
    "model.load_weights(os.path.join(cfg.paths.model_save_folder, cfg.model_name))\n",
    "logger.info(f\"Loaded model weights from {os.path.join(cfg.paths.model_save_folder, cfg.model_name)}\")\n",
    "\n",
    "classify = ClassifyGBF()\n",
    "starttime = UTCDateTime('2023-09-19T12:00:00')\n",
    "endtime = UTCDateTime('2023-09-19T16:00:00')\n",
    "tracedata, streams = classify.get_data_to_predict(starttime, endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12001\n",
      "trace shape pre t:  (3, 12001)\n",
      "trace shape post t:  (12001, 3)\n",
      "x shape:  (9601, 3)\n",
      "x shape:  (9601, 3)\n",
      "x shape:  (9601, 3)\n",
      "x shape:  (9601, 3)\n",
      "x shape:  (9601, 3)\n",
      "x shape:  (9601, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 40, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.converted_call(ag__.ld(len), (ag__.ld(self).conv_layers,), None, fscope) // 4,), None, fscope), None, loop_body_1, get_state_1, set_state_1, ('x',), {'iterate_names': 'i'})\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 35, in loop_body_1\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (4,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'j'})\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 34, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'cnn_dense' (type CNN_dense).\n    \n    in user code:\n    \n        File \"/staff/tord/Workspace/arces_classification/Classes/Models.py\", line 65, in call  *\n            x = layer(x, training=training)\n        File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"conv_block_0_conv\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 3)\n    \n    \n    Call arguments received by layer 'cnn_dense' (type CNN_dense):\n      • inputs=('tf.Tensor(shape=(None, 3), dtype=float32)',)\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/staff/tord/Workspace/arces_classification/live.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(tracedata[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m LiveClassifier(model, Scaler(), label_maps, cfg)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(tracedata[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/staff/tord/Workspace/arces_classification/live.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# TODO: Figure out the logic for this\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m yhats, yprobas, final_yhat, mean_proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensamble_predict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, X)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m final_yhat, mean_proba, yhats, yprobas, X\n",
      "\u001b[1;32m/staff/tord/Workspace/arces_classification/live.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m yhats, probas \u001b[39m=\u001b[39m [], {\u001b[39m\"\u001b[39m\u001b[39mdetector\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m: []}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     yhat, proba \u001b[39m=\u001b[39m one_prediction(model, x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_maps)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     yhats\u001b[39m.\u001b[39mappend(yhat)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     probas[\u001b[39m\"\u001b[39m\u001b[39mdetector\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(proba[\u001b[39m\"\u001b[39m\u001b[39mdetector\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Workspace/arces_classification/Classes/Utils.py:126\u001b[0m, in \u001b[0;36mone_prediction\u001b[0;34m(model, x, label_maps)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_prediction\u001b[39m(model, x, label_maps):\n\u001b[0;32m--> 126\u001b[0m     pred_probs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([x])\n\u001b[1;32m    127\u001b[0m     labels, pred_probs \u001b[39m=\u001b[39m get_final_labels(pred_probs, label_maps)\n\u001b[1;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m labels, pred_probs\n",
      "File \u001b[0;32m~/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filezjb3_pgg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7lygn2p.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m i \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mlen\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mconv_layers,), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m4\u001b[39;49m,), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m,), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[1;32m     41\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mflatten, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_2\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7lygn2p.py:35\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     33\u001b[0m     layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv_layers[\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(i) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(j)]\n\u001b[1;32m     34\u001b[0m     x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(layer), (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m---> 35\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mrange\u001b[39;49m), (\u001b[39m4\u001b[39;49m,), \u001b[39mNone\u001b[39;49;00m, fscope), \u001b[39mNone\u001b[39;49;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m,), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mj\u001b[39;49m\u001b[39m'\u001b[39;49m})\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mpool_layers[ag__\u001b[39m.\u001b[39mld(i)], (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7lygn2p.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body_1.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     32\u001b[0m j \u001b[39m=\u001b[39m itr\n\u001b[1;32m     33\u001b[0m layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mconv_layers[\u001b[39m4\u001b[39m \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(i) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(j)]\n\u001b[0;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(layer), (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(training)), fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 40, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.converted_call(ag__.ld(len), (ag__.ld(self).conv_layers,), None, fscope) // 4,), None, fscope), None, loop_body_1, get_state_1, set_state_1, ('x',), {'iterate_names': 'i'})\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 35, in loop_body_1\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (4,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'j'})\n    File \"/tmp/__autograph_generated_filer7lygn2p.py\", line 34, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'cnn_dense' (type CNN_dense).\n    \n    in user code:\n    \n        File \"/staff/tord/Workspace/arces_classification/Classes/Models.py\", line 65, in call  *\n            x = layer(x, training=training)\n        File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/staff/tord/miniconda3/envs/arces/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"conv_block_0_conv\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 3)\n    \n    \n    Call arguments received by layer 'cnn_dense' (type CNN_dense):\n      • inputs=('tf.Tensor(shape=(None, 3), dtype=float32)',)\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(len(tracedata[0][0]))\n",
    "model = LiveClassifier(model, Scaler(), label_maps, cfg)\n",
    "tracedata = np.array(tracedata)\n",
    "tracedata = tracedata.reshape(-1, tracedata.shape[1], 1)\n",
    "preds = model.predict(tracedata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(streams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arces_events = get_array_picks(UTCDateTime.now()-60000, UTCDateTime.now(), \"ARCES\")\n",
    "#starttimes, endtimes = transform_events_to_start_and_end_times(arces_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arces_events[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "array = \"arces\"\n",
    "detector_label_map = {0: \"noise\", 1: \"event\"}\n",
    "classifier_label_map = {0:\"earthquake\", 1:\"exlposion\"}\n",
    "label_maps = {\"detector\": detector_label_map, \"classifier\": classifier_label_map}\n",
    "\n",
    "\n",
    "input_shape = (cfg.live.length*cfg.live.sampling_rate, 3)\n",
    "detector_class_weight_dict = {\"noise\": 1, \"event\": 1}\n",
    "classifier_class_weight_dict = {\"earthquake\": 1, \"explosion\": 1}\n",
    "\n",
    "logger.info(\"Input shape to the model: \" + str(input_shape))\n",
    "classifier_metrics = [None]\n",
    "detector_metrics = [None]\n",
    "model = get_model(detector_label_map, classifier_label_map, detector_metrics, classifier_metrics, \n",
    "                   detector_class_weight_dict, classifier_class_weight_dict)\n",
    "model.build(input_shape=(None, *input_shape))  # Explicitly building the model here\n",
    "model.load_weights(os.path.join(cfg.paths.model_save_folder, cfg.model_name))\n",
    "\n",
    "scaler = Scaler()\n",
    "scaler.load_fitted()\n",
    "\n",
    "prediction = LiveClassifier(model, scaler, label_maps, cfg)\n",
    "\n",
    "times = ['2022-07-09T02:42:13',\n",
    "         '2022-04-24T00:58:40',\n",
    "         '2022-01-04T12:17:54',\n",
    "         '2022-01-04T12:12:11',\n",
    "         '2022-01-03T19:42:31',\n",
    "         '2021-11-28T15:39:09']\n",
    "\n",
    "times = list(map(UTCDateTime, times))\n",
    "\n",
    "for event_time in tqdm(times):\n",
    "    event_time_stripped = str(event_time.isoformat().split('.')[0].replace(':','')).strip()\n",
    "    step = 10\n",
    "    imgs = []\n",
    "    start = event_time - cfg.live.length\n",
    "    end = event_time + cfg.live.length\n",
    "    curr = start\n",
    "    try:\n",
    "        yhat, yproba, X = prediction.predict(start, end, step)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "for y, yp, x in zip(yhat, yproba, X):\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
