{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 15:56:56.810042: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-27 15:56:58.174096: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-27 15:56:59.587275: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-27 15:56:59.589485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-27 15:57:19.036238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from global_config import logger, cfg\n",
    "from Classes.Models import get_model\n",
    "import os\n",
    "import math\n",
    "from obspy import Trace, Stream, UTCDateTime\n",
    "from Classes.Scaler import Scaler\n",
    "from Classes.Utils import one_prediction\n",
    "from seismonpy.core import SeismonStream\n",
    "from seismonpy.norsardb import Client\n",
    "from seismonpy.utils import convert_velocity_slowness, create_global_mongodb_object\n",
    "from seismonpy.array_analysis.beams import array_beam\n",
    "from seismonpy.auto.beams.beam_types import VerticalCoherentBeamRecipe, HorizontalCoherentBeamRecipe\n",
    "from seismonpy.io.mongodb.eventdb import MongoEventDataBase\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveClassifier:\n",
    "    def __init__(self, model, scaler, label_maps, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.label_maps = label_maps\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, trace):\n",
    "        \"\"\"\n",
    "        Performs event classification for a specified time range.\n",
    "        Args:\n",
    "            start (UTCDateTime): Start time of the time range.\n",
    "            end (UTCDateTime): End time of the time range.\n",
    "            step (float): Step size in seconds.\n",
    "        Returns:\n",
    "            Predicted class.\n",
    "        \"\"\"\n",
    "        print(\"trace shape pre t: \", trace.shape)\n",
    "        trace = trace.T\n",
    "        print(\"trace shape post t: \", trace.shape)\n",
    "        X = self.prepare_multiple_intervals(trace)\n",
    "        for x in X:\n",
    "            print(\"x shape: \", x.shape)\n",
    "        X = [self.local_minmax(x) for x in X]\n",
    "        X = np.array(X)\n",
    "        # TODO: Figure out the logic for this\n",
    "        yhats, yprobas, final_yhat, mean_proba = self.ensamble_predict(self.model, X)\n",
    "\n",
    "        return final_yhat, mean_proba, yhats, yprobas, X\n",
    "    \n",
    "    def prepare_multiple_intervals(self, trace):\n",
    "        traces = []\n",
    "        # Creates equally sized intervals of the trace, using the user defined step size.\n",
    "        for start in range(0, len(trace) - (cfg.live.length*cfg.live.sample_rate)+1, (cfg.live.step*cfg.live.sample_rate)+1):\n",
    "            traces.append(trace[start:(start+cfg.live.length*cfg.live.sample_rate)+1])\n",
    "        return traces\n",
    "\n",
    "    def ensamble_predict(self, model, X):\n",
    "        # Basic ensamble prediction for the input data.\n",
    "        # TODO: Consider weighing predictions higher around the center (assuming thats where the pick is).\n",
    "        yhats, probas = [], {\"detector\": [], \"classifier\": []}\n",
    "        for x in X:\n",
    "            yhat, proba = one_prediction(model, x, self.label_maps)\n",
    "            yhats.append(yhat)\n",
    "            probas[\"detector\"].append(proba[\"detector\"])\n",
    "            probas[\"classifier\"].append(proba[\"classifier\"])\n",
    "        unqiue, counts = np.unique(yhats, return_counts=True)\n",
    "        final_yhat = unqiue[np.argmax(counts)]\n",
    "        mean_proba = {\"detector\": np.mean(probas[\"detector\"], axis=0), \"classifier\": np.mean(probas[\"classifier\"], axis=0)}\n",
    "        return yhats, probas, final_yhat, mean_proba\n",
    "    \n",
    "    def local_minmax(self, trace):\n",
    "        mmax = np.max(trace)\n",
    "        mmin = np.min(trace)\n",
    "        return (trace - mmin) / (mmax - mmin)\n",
    "        \n",
    "    \n",
    "class ClassifyGBF:\n",
    "\n",
    "    def __init__(self):\n",
    "        warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "    def get_beam(self, start, end, picks, inventory):\n",
    "        p_vel = cfg.live.p_vel\n",
    "        s_vel = cfg.live.s_vel\n",
    "        edge = cfg.live.edge\n",
    "        startt = start - edge\n",
    "        endt = end + edge\n",
    "        baz = self.average_bazimuth(picks)\n",
    "        try:\n",
    "            comp = 'BH*'\n",
    "            zcomp = '*Z'\n",
    "            tcomp = '*T'\n",
    "            rcomp = '*R'\n",
    "            if start < UTCDateTime('2014-09-19T00:00:00'):\n",
    "                comp = 's*'\n",
    "                zcomp = '*z'\n",
    "\n",
    "            stream = Client().get_waveforms(\n",
    "                'AR*', comp, starttime=startt - edge, endtime=endt + edge, sampling_rate_tolerance=0.5\n",
    "            )\n",
    "\n",
    "            stream = self.correct_trace_start_times(stream)\n",
    "\n",
    "            # Check for masked data, NaN values in traces\n",
    "            # Remove traces with more than 5 s masked\n",
    "            masked_traces = []\n",
    "            for tr in stream.traces:\n",
    "                if isinstance(tr.data, np.ma.masked_array):\n",
    "                    time_filled = tr.stats.delta* np.sum(tr.data.mask)\n",
    "                    if time_filled > 5.0:\n",
    "                        print(f'{time_filled:.4f} s of trace data masked, dropping trace - {tr.stats.starttime.__str__()}')\n",
    "                        masked_traces.append(tr)\n",
    "                    else:\n",
    "                        tr.data = tr.data.filled(0.0)\n",
    "                        print(f'{time_filled:.4f} s of trace data masked, filling with zeros - {tr.stats.starttime.__str__()}')\n",
    "\n",
    "                num_nans = np.sum(np.isnan(tr.data))\n",
    "                if num_nans > 0:\n",
    "                    time_containing_nans = num_nans*tr.stats.delta\n",
    "                    if time_containing_nans > 5.0:\n",
    "                        print(f'{time_containing_nans:.4f} s of trace has NaNs, dropping trace - {tr.stats.starttime.__str__()}')\n",
    "                        masked_traces.append(tr)\n",
    "                    else:\n",
    "                        tr.data = np.nan_to_num(tr.data)\n",
    "                        print(f'{time_containing_nans:.4f} s of trace has NaNs, filling with zeros - {tr.stats.starttime.__str__()}')\n",
    "\n",
    "\n",
    "            for tr in masked_traces:\n",
    "                if tr in stream.traces: # May have been removed already\n",
    "                    stream.remove(tr)\n",
    "            \n",
    "            if len(stream) == 0:\n",
    "                raise RuntimeError('Stream has no remaining traces')\n",
    "\n",
    "            stream.detrend('demean')\n",
    "            stream.taper(max_percentage=None, max_length=edge, type='cosine', halfcosine=True)\n",
    "            stream.filter('highpass', freq=1.5)\n",
    "            stream.resample(cfg.live.sample_rate)\n",
    "            #print(\"SAMPLE RATE:\", cfg.live.sample_rate)\n",
    "            stream.rotate('NE->RT', back_azimuth=baz, inventory=inventory)\n",
    "\n",
    "            p_time_delays = inventory.beam_time_delays(baz, p_vel)\n",
    "            p_beam_z = stream.select(channel=zcomp).create_beam(p_time_delays)\n",
    "            p_beam_z.stats.channel = 'P-beam, Z'\n",
    "\n",
    "            s_time_delays = inventory.beam_time_delays(baz, s_vel)\n",
    "            s_beam_t = stream.select(channel=tcomp).create_beam(s_time_delays)\n",
    "            s_beam_t.stats.channel = 'S-beam, T'\n",
    "            s_beam_r = stream.select(channel=rcomp).create_beam(s_time_delays)\n",
    "            s_beam_r.stats.channel = 'S-beam, R'\n",
    "\n",
    "            p_beam_z.trim(start, end)\n",
    "            s_beam_t.trim(start, end)\n",
    "            s_beam_r.trim(start, end)\n",
    "            \n",
    "            filter_name = cfg.filters.highpass_or_bandpass\n",
    "            if filter_name == \"highpass\":\n",
    "                stream.filter('highpass', freq = cfg.filters.high_kwargs.high_freq)\n",
    "            if filter_name == \"bandpass\":\n",
    "                stream.filter('bandpass', freqmin=cfg.filters.band_kwargs.min, freqmax=cfg.filters.band_kwargs.max)\n",
    "\n",
    "            stream = Stream([p_beam_z, s_beam_t, s_beam_r])\n",
    "            tracedata = np.array([p_beam_z.data, s_beam_t.data, s_beam_r.data])\n",
    "\n",
    "            return tracedata, stream\n",
    "\n",
    "        except Exception as exc:\n",
    "            print('ERROR: {} - {}'.format(start, exc))\n",
    "            return str(type(exc)) + str(exc), None\n",
    "\n",
    "    def correct_trace_start_times(self, stream, max_delta=0.15):\n",
    "        \"\"\"\n",
    "        For old data the traces might have tiny offset in start time, which breaks\n",
    "        beamforming. Adjust this manually.\n",
    "        Remove traces with diff > max_delta\n",
    "        \"\"\"\n",
    "        sts = [tr.stats.starttime for tr in stream.traces]\n",
    "        most_common = np.unique(sts)[0]\n",
    "\n",
    "        for tr in stream.traces:\n",
    "            this_starttime = tr.stats.starttime\n",
    "            if this_starttime != most_common:\n",
    "                if abs(this_starttime - most_common) <= max_delta:\n",
    "                    tr.stats.starttime = most_common\n",
    "                else:\n",
    "                    print('Removing trace:', tr)\n",
    "                    stream.remove(tr)\n",
    "        \n",
    "        return stream\n",
    "    \n",
    "    def get_data_to_predict(self, starttime, endtime):\n",
    "        filtered_events, inventory = self.get_array_picks(starttime, endtime, cfg.live.array)\n",
    "        print(\"Number of filtered events: \", len(filtered_events))\n",
    "        print(f\"Inventory: {inventory}\")\n",
    "        starttimes, endtimes = self.transform_events_to_start_and_end_times(filtered_events)\n",
    "        print(f\"starttimes: {starttimes}\")\n",
    "        tracedata, streams = [], []\n",
    "        for i, (starttime, endtime) in enumerate(zip(starttimes, endtimes)):\n",
    "            traced, stream = self.get_beam(starttime, endtime, filtered_events[i], inventory)\n",
    "            if traced is not isinstance(traced, str):\n",
    "                tracedata.append(traced)\n",
    "                streams.append(stream)\n",
    "        return tracedata, streams\n",
    "\n",
    "    \n",
    "    def average_bazimuth(self, picks):\n",
    "        bazimuths = [pick['backazimuth'] for pick in picks]\n",
    "        # Convert each azimuth to radians\n",
    "        radian_bazimuths = [math.radians(az) for az in bazimuths]\n",
    "        \n",
    "        # Calculate mean of sin and cos\n",
    "        mean_sin = sum(math.sin(az) for az in radian_bazimuths) / len(radian_bazimuths)\n",
    "        mean_cos = sum(math.cos(az) for az in radian_bazimuths) / len(radian_bazimuths)\n",
    "        \n",
    "        # Use atan2 to compute average azimuth in radians\n",
    "        average_bazimuth_rad = math.atan2(mean_sin, mean_cos)\n",
    "        \n",
    "        # Convert back to degrees, ensuring the result is within [0, 360)\n",
    "        average_bazimuth_deg = math.degrees(average_bazimuth_rad) % 360\n",
    "        \n",
    "        return average_bazimuth_deg\n",
    "    \n",
    "    def predict_gbf_event(self):\n",
    "        # Wrapper for predict function that handles GBF picks.\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def load_events(self, starttime: UTCDateTime, endtime: UTCDateTime, collection: str = \"gbf1440_large\", dbname: str = \"auto\",  \n",
    "        mongourl: str = \"mongo.norsar.no:27017\", mongodb_user: str = \"guest\", mongodb_password: str = \"guest\", \n",
    "        mongodb_authsource: str = \"test\"):\n",
    "        query = {\"$and\":\n",
    "            [\n",
    "                {\"origins.time\": {\"$gt\": starttime.isoformat()}},\n",
    "                {\"origins.time\": {\"$lt\": endtime.isoformat()}},\n",
    "                {\"picks.waveform_id.station_code\": \"ARCES\"}\n",
    "            ]\n",
    "            }\n",
    "        obj = create_global_mongodb_object(mongourl.split(\":\")[0], int(mongourl.split(\":\")[1]), mongodb_user, mongodb_password, mongodb_authsource)\n",
    "        db = MongoEventDataBase(obj[dbname], collection)\n",
    "        events = db.find_events(query, decode_result=True)\n",
    "        inventory = Client().get_array_inventory(cfg.live.array)\n",
    "        return events, inventory\n",
    "\n",
    "    def get_array_picks(self, starttime: UTCDateTime, endtime:UTCDateTime, station_code: str):\n",
    "        events, inventory = self.load_events(starttime, endtime)\n",
    "        # Filter events where ARCES made a detection\n",
    "        relevant_events = [event for event in events if any(pick.waveform_id.station_code == station_code for pick in event.picks)]\n",
    "        \n",
    "        # Extract only the ARCES-related picks from those events\n",
    "        nested_filtered_events = []\n",
    "        for event in relevant_events:\n",
    "            arces_picks = [pick for pick in event.picks if pick.waveform_id.station_code == station_code]\n",
    "            nested_filtered_events.append(arces_picks)\n",
    "            \n",
    "        return nested_filtered_events, inventory\n",
    "\n",
    "    def transform_events_to_start_and_end_times(self, filtered_events: list):\n",
    "        starttimes, endtimes = [], []\n",
    "        for event in filtered_events:\n",
    "            pick_times = [pick.time for pick in event]\n",
    "            start = min(pick_times)\n",
    "            end = max(pick_times)\n",
    "            duration = end - start\n",
    "            # Need to make sure the event is long enough to include the entire event + event buffer. \n",
    "            # Also \n",
    "            if duration < cfg.live.length:\n",
    "                missing_length = cfg.live.length - duration\n",
    "                start = start - missing_length/2\n",
    "                end = end + missing_length/2\n",
    "            # How to handle events that are too long for the model? \n",
    "            starttimes.append(start - cfg.live.event_buffer)\n",
    "            endtimes.append(end + cfg.live.event_buffer)\n",
    "\n",
    "        return starttimes, endtimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mARCES - 1223366037.py:10 - <module> - INFO: Input shape to the model: (9601, 3)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mARCES - 1223366037.py:17 - <module> - INFO: Loaded model weights from /staff/tord/Workspace/arces_classification/output/models/cnn_dense_mute-sky-7934_20231013_090836_model_weights.h5\u001b[0m\n",
      "Number of filtered events:  0\n",
      "Inventory: Inventory created at 2023-10-27T14:10:16.333187Z\n",
      "\tCreated by: ObsPy 1.4.0\n",
      "\t\t    https://www.obspy.org\n",
      "\tSending institution: NO\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tNO\n",
      "\t\tStations (26):\n",
      "\t\t\tNO.ARA0 ()\n",
      "\t\t\tNO.ARA1 ()\n",
      "\t\t\tNO.ARA2 ()\n",
      "\t\t\tNO.ARA3 ()\n",
      "\t\t\tNO.ARB1 ()\n",
      "\t\t\tNO.ARB2 ()\n",
      "\t\t\tNO.ARB3 ()\n",
      "\t\t\tNO.ARB4 ()\n",
      "\t\t\tNO.ARB5 ()\n",
      "\t\t\tNO.ARC1 ()\n",
      "\t\t\tNO.ARC2 ()\n",
      "\t\t\tNO.ARC3 ()\n",
      "\t\t\tNO.ARC4 ()\n",
      "\t\t\tNO.ARC5 ()\n",
      "\t\t\tNO.ARC6 ()\n",
      "\t\t\tNO.ARC7 ()\n",
      "\t\t\tNO.ARD1 ()\n",
      "\t\t\tNO.ARD2 ()\n",
      "\t\t\tNO.ARD3 ()\n",
      "\t\t\tNO.ARD4 ()\n",
      "\t\t\tNO.ARD5 ()\n",
      "\t\t\tNO.ARD6 ()\n",
      "\t\t\tNO.ARD7 ()\n",
      "\t\t\tNO.ARD8 ()\n",
      "\t\t\tNO.ARD9 ()\n",
      "\t\t\tNO.ARE0 ()\n",
      "\t\tChannels (280):\n",
      "\t\t\tNO.ARA0..BDF, NO.ARA0..BHZ, NO.ARA0..BHN, NO.ARA0..BHE, \n",
      "\t\t\tNO.ARA0..HHZ, NO.ARA0..HHN, NO.ARA0..HHE, NO.ARA0.SE.SHE (3x), \n",
      "\t\t\tNO.ARA0.SN.SHN (3x), NO.ARA0.SZ.SHZ (3x), NO.ARA1..BDF (2x), \n",
      "\t\t\tNO.ARA1..BHZ, NO.ARA1..BHN, NO.ARA1..BHE, NO.ARA1..HHZ, \n",
      "\t\t\tNO.ARA1..HHN, NO.ARA1..HHE, NO.ARA1.SZ.SHZ (3x), NO.ARA2..BDF (2x)\n",
      "\t\t\tNO.ARA2..BHZ, NO.ARA2..BHN, NO.ARA2..BHE, NO.ARA2..HHZ, \n",
      "\t\t\tNO.ARA2..HHN, NO.ARA2..HHE, NO.ARA2.SZ.SHZ (3x), NO.ARA3..BDF, \n",
      "\t\t\tNO.ARA3..HHZ, NO.ARA3..HHN, NO.ARA3..HHE, NO.ARB1..BDF, \n",
      "\t\t\tNO.ARB1..HHZ, NO.ARB1..HHN, NO.ARB1..HHE, NO.ARB2..BDF (2x), \n",
      "\t\t\tNO.ARB2..BHZ, NO.ARB2..BHN, NO.ARB2..BHE, NO.ARB2..HHZ, \n",
      "\t\t\tNO.ARB2..HHN, NO.ARB2..HHE, NO.ARB2.SZ.SHZ (3x), NO.ARB3..BDF (2x)\n",
      "\t\t\tNO.ARB3..BHZ, NO.ARB3..BHN, NO.ARB3..BHE, NO.ARB3..HHZ, \n",
      "\t\t\tNO.ARB3..HHN, NO.ARB3..HHE, NO.ARB3.SZ.SHZ (3x), NO.ARB4..BDF, \n",
      "\t\t\tNO.ARB4..HHZ, NO.ARB4..HHN, NO.ARB4..HHE, NO.ARB5..BDF, \n",
      "\t\t\tNO.ARB5..HHZ, NO.ARB5..HHN, NO.ARB5..HHE, NO.ARC1..BHZ, \n",
      "\t\t\tNO.ARC1..BHN, NO.ARC1..BHE, NO.ARC1..HHZ, NO.ARC1..HHN, \n",
      "\t\t\tNO.ARC1..HHE, NO.ARC1.SZ.SHZ (3x), NO.ARC2..BHZ (2x), \n",
      "\t\t\tNO.ARC2..BHN (2x), NO.ARC2..BHE (2x), NO.ARC2..HHZ, NO.ARC2..HHN, \n",
      "\t\t\tNO.ARC2..HHE, NO.ARC2.SE.SHE (3x), NO.ARC2.SN.SHN (3x), \n",
      "\t\t\tNO.ARC2.SZ.SHZ (3x), NO.ARC3..BHZ (2x), NO.ARC3..BHN (2x), \n",
      "\t\t\tNO.ARC3..BHE (2x), NO.ARC3..HHZ, NO.ARC3..HHN, NO.ARC3..HHE, \n",
      "\t\t\tNO.ARC3.SZ.SHZ (3x), NO.ARC4..BHZ, NO.ARC4..BHN, NO.ARC4..BHE, \n",
      "\t\t\tNO.ARC4..HHZ, NO.ARC4..HHN, NO.ARC4..HHE, NO.ARC4.SE.SHE (3x), \n",
      "\t\t\tNO.ARC4.SN.SHN (3x), NO.ARC4.SZ.SHZ (3x), NO.ARC5..BHZ, \n",
      "\t\t\tNO.ARC5..BHN, NO.ARC5..BHE, NO.ARC5..HHZ, NO.ARC5..HHN, \n",
      "\t\t\tNO.ARC5..HHE, NO.ARC5.SZ.SHZ (3x), NO.ARC6..BHZ, NO.ARC6..BHN, \n",
      "\t\t\tNO.ARC6..BHE, NO.ARC6..HHZ, NO.ARC6..HHN, NO.ARC6..HHE, \n",
      "\t\t\tNO.ARC6.SZ.SHZ (3x), NO.ARC7..BHZ, NO.ARC7..BHN, NO.ARC7..BHE, \n",
      "\t\t\tNO.ARC7..HHZ, NO.ARC7..HHN, NO.ARC7..HHE, NO.ARC7.SE.SHE (3x), \n",
      "\t\t\tNO.ARC7.SN.SHN (3x), NO.ARC7.SZ.SHZ (3x), NO.ARD1..BHZ, \n",
      "\t\t\tNO.ARD1..BHN, NO.ARD1..BHE, NO.ARD1..HHZ, NO.ARD1..HHN, \n",
      "\t\t\tNO.ARD1..HHE, NO.ARD1.SZ.SHZ (3x), NO.ARD2..BHZ, NO.ARD2..BHN, \n",
      "\t\t\tNO.ARD2..BHE, NO.ARD2..HHZ, NO.ARD2..HHN, NO.ARD2..HHE, \n",
      "\t\t\tNO.ARD2.SZ.SHZ (3x), NO.ARD3..BHZ, NO.ARD3..BHN, NO.ARD3..BHE, \n",
      "\t\t\tNO.ARD3..HHZ, NO.ARD3..HHN, NO.ARD3..HHE, NO.ARD3.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARD4..BHZ, NO.ARD4..BHN, NO.ARD4..BHE, NO.ARD4..HHZ, \n",
      "\t\t\tNO.ARD4..HHN, NO.ARD4..HHE, NO.ARD4.SZ.SHZ (3x), NO.ARD5..BHZ, \n",
      "\t\t\tNO.ARD5..BHN, NO.ARD5..BHE, NO.ARD5..HHZ, NO.ARD5..HHN, \n",
      "\t\t\tNO.ARD5..HHE, NO.ARD5.SZ.SHZ (3x), NO.ARD6..BHZ (3x), \n",
      "\t\t\tNO.ARD6..BHN (3x), NO.ARD6..BHE (3x), NO.ARD6..HHZ, NO.ARD6..HHN, \n",
      "\t\t\tNO.ARD6..HHE, NO.ARD6.SZ.SHZ (3x), NO.ARD7..BHZ, NO.ARD7..BHN, \n",
      "\t\t\tNO.ARD7..BHE, NO.ARD7..HHZ, NO.ARD7..HHN, NO.ARD7..HHE, \n",
      "\t\t\tNO.ARD7.SZ.SHZ (3x), NO.ARD8..BHZ, NO.ARD8..BHN, NO.ARD8..BHE, \n",
      "\t\t\tNO.ARD8..HHZ, NO.ARD8..HHN, NO.ARD8..HHE, NO.ARD8.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARD9..BHZ (2x), NO.ARD9..BHN (2x), NO.ARD9..BHE (2x), \n",
      "\t\t\tNO.ARD9..HHZ, NO.ARD9..HHN, NO.ARD9..HHE, NO.ARD9.SZ.SHZ (3x), \n",
      "\t\t\tNO.ARE0..BHZ, NO.ARE0..BHN, NO.ARE0..BHE, NO.ARE0.BE.BHE (4x), \n",
      "\t\t\tNO.ARE0.BN.BHN (4x), NO.ARE0.BZ.BHZ (4x), NO.ARE0.HE.HHE, \n",
      "\t\t\tNO.ARE0.HN.HHN, NO.ARE0.HZ.HHZ, NO.ARE0.IE.MHE, NO.ARE0.IN.MHN, \n",
      "\t\t\tNO.ARE0.IZ.MHZ, NO.ARE0.LE.LHE, NO.ARE0.LN.LHN, NO.ARE0.LZ.LHZ, \n",
      "\t\t\tNO.ARE0.XH.EHZ, NO.ARE0.XH.EHN, NO.ARE0.XH.EHE\n",
      "starttimes: []\n"
     ]
    }
   ],
   "source": [
    "detector_label_map = {0: \"noise\", 1: \"event\"}\n",
    "classifier_label_map = {0:\"earthquake\", 1:\"exlposion\"}\n",
    "label_maps = {\"detector\": detector_label_map, \"classifier\": classifier_label_map}\n",
    "\n",
    "\n",
    "input_shape = (cfg.live.length*cfg.live.sample_rate + 1, 3)\n",
    "detector_class_weight_dict = {\"noise\": 1, \"event\": 1}\n",
    "classifier_class_weight_dict = {\"earthquake\": 1, \"explosion\": 1}\n",
    "\n",
    "logger.info(\"Input shape to the model: \" + str(input_shape))\n",
    "classifier_metrics = [None]\n",
    "detector_metrics = [None]\n",
    "model = get_model(detector_label_map, classifier_label_map, detector_metrics, classifier_metrics, \n",
    "                   detector_class_weight_dict, classifier_class_weight_dict)\n",
    "model.build(input_shape=(None, *input_shape))  # Explicitly building the model here\n",
    "model.load_weights(os.path.join(cfg.paths.model_save_folder, cfg.model_name))\n",
    "logger.info(f\"Loaded model weights from {os.path.join(cfg.paths.model_save_folder, cfg.model_name)}\")\n",
    "\n",
    "classify = ClassifyGBF()\n",
    "\n",
    "starttime = UTCDateTime('2020-10-30T14:01:00')\n",
    "endtime =   UTCDateTime('2020-11-07T16:45:00')\n",
    "tracedata, streams = classify.get_data_to_predict(starttime, endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/staff/tord/Workspace/arces_classification/live.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(tracedata[\u001b[39m0\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m LiveClassifier(model, Scaler(), label_maps, cfg)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m final_classifications \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(len(tracedata[0]))\n",
    "model = LiveClassifier(model, Scaler(), label_maps, cfg)\n",
    "final_classifications = []\n",
    "mean_probas = []\n",
    "for trace in tracedata:\n",
    "    final_yhat, mean_proba, yhats, yprobas, _ = model.predict(trace)\n",
    "    final_classifications.append(final_yhat)\n",
    "    mean_probas.append(mean_proba)\n",
    "\n",
    "print(\"All classifications:\", final_classifications)\n",
    "print(\"Mean probas:\", mean_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/staff/tord/Workspace/arces_classification/live.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsaturn/staff/tord/Workspace/arces_classification/live.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m preds\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
